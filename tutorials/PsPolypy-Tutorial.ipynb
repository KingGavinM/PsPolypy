{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for PsPolypy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for this example\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage.util import img_as_float\n",
    "from skimage.color import rgb2gray, rgba2rgb\n",
    "import scipy\n",
    "\n",
    "from IPython.display import display, Math\n",
    "\n",
    "# Locate PsPolypy. Only necessary if PsPolypy is not installed as a package.\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "# Import PsPolypy\n",
    "import  PsPolypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "\n",
    "# rcParams settings\n",
    "\n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color='krbmgcy')\n",
    "\n",
    "plt.rcParams['axes.linewidth'] = 1.5\n",
    "\n",
    "plt.rcParams['xtick.top'] = plt.rcParams['ytick.right'] = True\n",
    "plt.rcParams['xtick.minor.visible'] = plt.rcParams['ytick.minor.visible'] = True\n",
    "plt.rcParams['xtick.direction'] = plt.rcParams['ytick.direction'] = 'in'\n",
    "plt.rcParams['xtick.major.size'] = plt.rcParams['ytick.major.size'] = 8 \n",
    "plt.rcParams['xtick.minor.size'] = plt.rcParams['ytick.minor.size'] = 5\n",
    "plt.rcParams['xtick.major.width'] = plt.rcParams['ytick.major.width'] = 1; \n",
    "plt.rcParams['savefig.bbox']='tight'\n",
    "plt.rcParams['savefig.transparent']=True\n",
    "\n",
    "plt.rcParams['legend.framealpha'] = 0.6\n",
    "plt.rcParams['legend.markerscale'] = 2.\n",
    "plt.rcParams['legend.fontsize'] = 'xx-large'\n",
    "plt.rcParams['figure.subplot.wspace'] = 0.02\n",
    "plt.rcParams['figure.subplot.hspace'] = 0.02\n",
    "plt.rcParams['axes.labelpad'] = 5.\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = plt.rcParams['ytick.labelsize'] = 20\n",
    "plt.rcParams['axes.labelsize'] = 24\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "\n",
    "plt.rcParams[\"font.family\"]='serif'\n",
    "plt.rcParams[\"mathtext.fontset\"]='stix';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Polydat Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Polydat``` is the main object that stores and processes image fields containing polymer particles. The object can handle any number of full field images, provided they are the same nm/px resolution. A general outline of ```Polydat```'s properties is shown here:\n",
    "\n",
    "```project\n",
    "Polydat Properties\n",
    "├─ images (list[np.ndarray])\n",
    "│    └─ A list of full-field image arrays (2D numpy arrays) loaded into the object.\n",
    "│\n",
    "├─ resolution (float)\n",
    "│    └─ The image resolution in nanometers per pixel.\n",
    "│\n",
    "├─ metadata (Dict[str, Any])\n",
    "│    └─ A dictionary containing metadata for the experiment or data file\n",
    "│         (e.g., {'recorded_on': '2024-12-25'}).\n",
    "│\n",
    "├─ particles (list[Particle])\n",
    "│    └─ A list of Particle objects representing detected polymer particles.\n",
    "│         Populated by the segment_particles() method.\n",
    "│\n",
    "├─ included_classifications (list[str])\n",
    "│    └─ A list of particle classifications currently included in the analysis\n",
    "│         (e.g., ['Linear', 'Branched', …]).\n",
    "│         Updated by filter_particles().\n",
    "│\n",
    "├─ num_particles (Dict[str, int])\n",
    "│    └─ A dictionary with counts of particles by classification\n",
    "│         (e.g., {\"All\": 12, \"Linear\": 5, …}).\n",
    "│         Updated when particles are segmented and classified.\n",
    "│\n",
    "├─ contour_lengths (list[float])\n",
    "│    └─ A list of all contour lengths (in nm) from every skeletonized particle branch.\n",
    "│         Set by interpolate_skeletons().\n",
    "│\n",
    "├─ contour_sampling (np.ndarray)\n",
    "│    └─ A 1D array of contour-length sample points (from 0 to max length)\n",
    "│         used for interpolating skeleton coordinates.\n",
    "│         Set by interpolate_skeletons().\n",
    "│\n",
    "├─ mean_squared_displacements (np.ndarray)\n",
    "│    └─ A 1D array representing the average squared displacements for all interpolated subpaths,\n",
    "│         plotted versus contour length.\n",
    "│\n",
    "├─ mean_tantan_correlation (np.ndarray)\n",
    "│    └─ A 1D array containing the mean tangent–tangent correlation of all skeleton paths at each contour-length\n",
    "│         separation. Calculated by calc_tantan_correlations().\n",
    "│\n",
    "├─ displacement_sq_confidence_intervals (np.ndarray)\n",
    "│    └─ A 2D array (N×2) providing lower/upper confidence bounds for mean squared displacements\n",
    "│         at each contour-length step.\n",
    "│\n",
    "├─ tantan_confidence_intervals (np.ndarray)\n",
    "│    └─ A 2D array (N×2) providing lower/upper confidence bounds for mean tan–tan correlations\n",
    "│         at each contour-length step.\n",
    "│\n",
    "├─ R2_fit_result (lmfit.model.ModelResult)\n",
    "│    └─ The lmfit ModelResult from fitting the end-to-end distance squared model.\n",
    "│         The best-fit persistence length (lp) can be retrieved via R2_fit_result.params['lp'].value.\n",
    "│\n",
    "├─ tantan_fit_result (lmfit.model.ModelResult)\n",
    "│    └─ The lmfit ModelResult from fitting the tan–tan correlation model.\n",
    "│         The best-fit persistence length (lp) is available as tantan_fit_result.params['lp'].value.\n",
    "│\n",
    "├─ min_fitting_length (float)\n",
    "│    └─ The lower bound (in nm) of the contour-length region used when fitting\n",
    "│         persistence-length models.\n",
    "│\n",
    "└─ max_fitting_length (float)\n",
    "     └─ The upper bound (in nm) of the contour-length region used for fitting.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once an instance of ```polydat``` has been initialized, many different analysis methods can be performed. An overview of the available methods is shown here.\n",
    "\n",
    "```project\n",
    "Polydat Methods\n",
    "├─ __init__(images: list[np.ndarray] = None, resolution: float = None, **metadata: Any) -> None\n",
    "│    Initialize a new Polydat instance with images, resolution, and metadata.\n",
    "│\n",
    "├─ Static Methods\n",
    "│    ├─ __load_with_skimage(filepath: str) -> np.ndarray\n",
    "│    │      Loads an image file using skimage.io.imread, converts it to grayscale,\n",
    "│    │      and handles channel conversion (RGBA/RGB) as needed.\n",
    "│    │\n",
    "│    ├─ __exponential_model(x, lp)\n",
    "│    │      Implements an exponential decay model used for tan–tan correlation fitting.\n",
    "│    │\n",
    "│    └─ __R2_model(x, lp)\n",
    "│           Implements the end-to-end distance squared model used for persistence length fitting.\n",
    "│\n",
    "├─ Class Methods\n",
    "│    └─ from_images(filepaths: list[str], resolution: float, **metadata) -> Polydat\n",
    "│           Create a Polydat instance from a list of image file paths by loading each image.\n",
    "│\n",
    "├─ Main Methods\n",
    "│    ├─ add_image(filepath: str, resolution: float) -> list[np.ndarray]\n",
    "│    │      Loads an image from the given file and appends it to the image list.\n",
    "│    │\n",
    "│    ├─ upscale(magnification: float, order: int = 3) -> Tuple[list[np.ndarray], float]\n",
    "│    │      Upscales all images by the specified magnification factor (using interpolation) and\n",
    "│    │      updates the resolution and metadata.\n",
    "│    │\n",
    "│    ├─ segment_particles(minimum_area: int = 10, padding: int = 1) -> list[Particle]\n",
    "│    │      Segments particles from full-field images via thresholding and connected region labeling;\n",
    "│    │      creates and stores Particle objects.\n",
    "│    │\n",
    "│    ├─ skeletonize_particles(method: str = 'zhang') -> list[Particle]\n",
    "│    │      Skeletonizes each Particle’s image; removes particles with insufficient skeleton data.\n",
    "│    │\n",
    "│    ├─ classify_particles() -> list[Particle]\n",
    "│    │      Classifies each Particle (e.g., Linear, Branched, etc.) and updates particle counts.\n",
    "│    │\n",
    "│    ├─ filter_particles(classifications: list[str]) -> list[Particle]\n",
    "│    │      Filters the Particle list to include only those with specified classifications.\n",
    "│    │\n",
    "│    ├─ interpolate_skeletons(step_size: float, k: int = 3, s: float = 0.5) -> None\n",
    "│    │      Interpolates the skeletons for each particle (using spline interpolation) to obtain\n",
    "│    │      subpixel contour coordinates.\n",
    "│    │\n",
    "│    ├─ calc_displacements() -> None\n",
    "│    │      Computes the mean squared displacements along each interpolated path from all particles,\n",
    "│    │      including calculation of standard deviations and SEM for error bars.\n",
    "│    │\n",
    "│    ├─ calc_tantan_correlations() -> None\n",
    "│    │      Computes the tangent–tangent correlations (and associated error metrics) from each particle.\n",
    "│    │\n",
    "│    ├─ calc_R2_lp(lp_init: float = 10,\n",
    "│    │              min_fitting_length: float = 0,\n",
    "│    │              max_fitting_length: float = np.inf,\n",
    "│    │              **fit_kwargs) -> None\n",
    "│    │      Fits the end-to-end distance squared (R²) model to the mean squared displacement data\n",
    "│    │      to estimate the persistence length (lp) using lmfit.\n",
    "│    │\n",
    "│    └─ calc_tantan_lp(lp_init: float = 10,\n",
    "│                   min_fitting_length: float = 0,\n",
    "│                   max_fitting_length: float = np.inf,\n",
    "│                   **fit_kwargs) -> None\n",
    "│           Fits the exponential decay (tan–tan correlation) model to estimate the persistence length.\n",
    "│\n",
    "├─ Plotting Methods\n",
    "│    ├─ plot_image(index: int = 0, ax: plt.Axes = None, **kwargs) -> plt.Axes\n",
    "│    │      Plots one of the full-field images.\n",
    "│    │\n",
    "│    ├─ plot_particle(index: int, ax: plt.Axes = None, **kwargs) -> plt.Axes\n",
    "│    │      Plots the image and details for a specific Particle.\n",
    "│    │\n",
    "│    ├─ plot_skeleton(index: int, ax: plt.Axes = None, **kwargs) -> plt.Axes\n",
    "│    │      Plots the skeleton (binary representation) of a given Particle.\n",
    "│    │\n",
    "│    ├─ plot_interpolated_skeleton(index: int, ax: plt.Axes = None, **kwargs) -> plt.Axes\n",
    "│    │      Plots the interpolated skeleton (smooth path) of a Particle.\n",
    "│    │\n",
    "│    ├─ plot_contour_distribution(n_points: int = 100, ax: plt.Axes = None, …) -> plt.Axes\n",
    "│    │      Uses Gaussian KDE to plot the distribution of all contour lengths from particle branches,\n",
    "│    │      highlighting the included versus excluded fitting ranges.\n",
    "│    │\n",
    "│    ├─ plot_subpath_contour_distribution(n_points: int = 100, ax: plt.Axes = None, …) -> plt.Axes\n",
    "│    │      Similar to plot_contour_distribution but for subpath contour lengths.\n",
    "│    │\n",
    "│    ├─ plot_mean_squared_displacements(error_bars: bool = False,\n",
    "│    │                                    ax: plt.Axes = None,\n",
    "│    │                                    inc_kwargs: dict = None,\n",
    "│    │                                    exc_kwargs: dict = None,\n",
    "│    │                                    vline_kwargs: dict = None) -> plt.Axes\n",
    "│    │      Plots the mean squared displacements versus contour length with optional error bars,\n",
    "│    │      and distinguishes data within the fitting range.\n",
    "│    │\n",
    "│    ├─ plot_mean_squared_displacements_fit(ax: plt.Axes = None,\n",
    "│    │                                         show_init: bool = False,\n",
    "│    │                                         fit_kwargs: dict = None,\n",
    "│    │                                         init_kwargs: dict = None) -> plt.Axes\n",
    "│    │      Overlays the fitted R² model (and optionally the initial guess) on the displacement data.\n",
    "│    │\n",
    "│    ├─ plot_mean_tantan_correlations(error_bars: bool = False,\n",
    "│    │                                  ax: plt.Axes = None,\n",
    "│    │                                  inc_kwargs: dict = None,\n",
    "│    │                                  exc_kwargs: dict = None,\n",
    "│    │                                  vline_kwargs: dict = None) -> plt.Axes\n",
    "│    │      Plots the mean tangent–tangent correlations with optional error bars,\n",
    "│    │      differentiating between included and excluded data.\n",
    "│    │\n",
    "│    └─ plot_mean_tantan_correlations_fit(ax: plt.Axes = None,\n",
    "│                                          show_init: bool = False,\n",
    "│                                          fit_kwargs: dict = None,\n",
    "│                                          init_kwargs: dict = None) -> plt.Axes\n",
    "│           Overlays the fitted exponential decay (tan–tan) model on the correlation data.\n",
    "│\n",
    "├─ Printing/Summary Methods\n",
    "│    ├─ print_image_summary() -> None\n",
    "│    │      Prints a summary of the image data (number of images, resolution, and upscaling details).\n",
    "│    │\n",
    "│    ├─ print_classification_summary() -> None\n",
    "│    │      Prints a summary of particle segmentation and classification counts.\n",
    "│    │\n",
    "│    ├─ print_pl_summary() -> None\n",
    "│    │      Prints a summary of persistence length (lp) calculations, including fit details.\n",
    "│    │\n",
    "│    └─ print_summary() -> None\n",
    "│           Prints a complete summary combining image, classification, and persistence length data.\n",
    "│\n",
    "└─ Data Extraction Methods\n",
    "     ├─ squared_displacements_at_lag(lag: int = 0) -> np.ndarray\n",
    "     │      Returns an array of squared displacements for a specified lag.\n",
    "     │\n",
    "     └─ tantan_correlations_at_lag(lag: int = 0) -> np.ndarray\n",
    "            Returns an array of tan–tan correlations for a specified lag.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Particle Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During ```polydat``` processing, ```particle``` images are made each encapsulating a feature detectedin the polymer field image. An overview of available properties of the ```particle``` object is shown here.\n",
    "```project\n",
    "Particle Properties\n",
    "├─ image (np.ndarray)\n",
    "│    └─ The image of the particle.\n",
    "│\n",
    "├─ resolution (float)\n",
    "│    └─ The resolution of the image in nanometers per pixel.\n",
    "│\n",
    "├─ bbox (Tuple[int, int, int, int])\n",
    "│    └─ The bounding box coordinates of the particle in the full field image.\n",
    "│\n",
    "├─ binary_mask (np.ndarray)\n",
    "│    └─ The binary mask of the particle.\n",
    "│\n",
    "├─ classification (str)\n",
    "│    └─ The classification of the particle (e.g., Linear, Branched, Looped, Branched-Looped, or Unknown).\n",
    "│\n",
    "├─ id (int)\n",
    "│    └─ The unique identifier for the particle.\n",
    "│\n",
    "├─ skeleton (skan.Skeleton)\n",
    "│    └─ The skeleton representation of the particle, generated from its binary mask.\n",
    "│\n",
    "├─ skeleton_summary (pandas.DataFrame)\n",
    "│    └─ A summary of the skeleton properties (as generated by skan) used for classification.\n",
    "│\n",
    "├─ contour_samplings (list[np.ndarray])\n",
    "│    └─ A list of contour sampling arrays for each skeleton path (the sampled positions along each contour).\n",
    "│\n",
    "├─ contour_lengths (list[float])\n",
    "│    └─ The contour lengths (in nm) of each skeleton path.\n",
    "│\n",
    "├─ interp_skeleton_coordinates (list[np.ndarray])\n",
    "│    └─ The interpolated skeleton coordinates for each path (providing subpixel accuracy).\n",
    "│\n",
    "├─ interp_skeleton_derivatives (list[np.ndarray])\n",
    "│    └─ The derivatives of the interpolated skeleton paths, used for tangent calculations.\n",
    "│\n",
    "├─ displacements (list[np.ndarray])\n",
    "│    └─ The displacements calculated along each skeleton path.\n",
    "│\n",
    "└─ tantan_correlations (list[np.ndarray])\n",
    "     └─ The tangent–tangent (Tan-Tan) correlations computed for each path.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```project\n",
    "Particle Methods\n",
    "├─ __init__(image: np.ndarray, resolution: float, bbox: Tuple[int, int, int, int],\n",
    "│           binary_mask: np.ndarray = None, classification: str = None, id: int = None) -> None\n",
    "│         Initialize a Particle object by setting the image, resolution, bounding box, binary mask,\n",
    "│         classification, and particle ID. Also initializes additional processing attributes.\n",
    "│\n",
    "├─ Main Processing Methods\n",
    "│    ├─ skeletonize_particle(method: str = 'zhang') -> Tuple[skan.Skeleton, pandas.DataFrame]\n",
    "│    │      Skeletonize the particle's binary mask using the specified method, set the skeleton and\n",
    "│    │      skeleton_summary attributes, and return both.\n",
    "│    │\n",
    "│    ├─ classify() -> str\n",
    "│    │      Classify the particle (e.g., as Linear, Branched, Looped, Branched-Looped, or Unknown)\n",
    "│    │      based on its skeleton summary; update and return the classification.\n",
    "│    │\n",
    "│    ├─ interpolate_skeleton(step_size: float, k: int = 3, s: float = 0.5)\n",
    "│    │      -> Tuple[list[np.ndarray], list[float], list[np.ndarray], list[np.ndarray]]\n",
    "│    │      Interpolate each skeleton path using spline interpolation (with the specified step size,\n",
    "│    │      degree *k*, and smoothing factor *s*) to compute:\n",
    "│    │          - Contour samplings,\n",
    "│    │          - Contour lengths,\n",
    "│    │          - Interpolated skeleton coordinates, and\n",
    "│    │          - Interpolated skeleton derivatives.\n",
    "│    │      Set the corresponding attributes and return these values.\n",
    "│    │\n",
    "│    ├─ calc_displacements() -> list[np.ndarray]\n",
    "│    │      Compute displacement matrices for each interpolated skeleton path, extract subpath\n",
    "│    │      displacements, and return a list of displacement arrays.\n",
    "│    │\n",
    "│    └─ calc_tantan_correlation() -> list[np.ndarray]\n",
    "│           Compute the tangent–tangent (Tan-Tan) correlations for each interpolated path based on\n",
    "│           the unit tangent vectors and return a list of correlation arrays.\n",
    "│\n",
    "└─ Plotting Methods\n",
    "     ├─ plot_particle(ax: plt.Axes = None, **kwargs) -> plt.Axes\n",
    "     │      Plot the particle's image on a given (or current) matplotlib axis.\n",
    "     │\n",
    "     ├─ plot_skeleton(ax: plt.Axes = None, **kwargs) -> plt.Axes\n",
    "     │      Plot the particle’s skeleton (from the skeleton attribute) on a matplotlib axis.\n",
    "     │\n",
    "     └─ plot_interpolated_skeleton(ax: plt.Axes = None, **kwargs) -> plt.Axes\n",
    "            Plot the spline-interpolated skeleton paths (using the interpolated coordinates)\n",
    "            on a matplotlib axis.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-By-Step Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an instance of Polydat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three means of creating an instance of the ```Polydat``` class.```from_images``` for creates an instance of ```Polydat``` from a list of image files. An empty ```Polydat``` instance can be created, and images can be added with the ```add_image``` method. Finally, a list of images can be passed to the built-in initialization function for ```polydat```. These options are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Polydat class from a single image. Note, it must be a list of file paths, even if there is only one image.\n",
    "image_path = ['example_images/exampleCL0.png']\n",
    "polydat = PsPolypy.Polymer.Polydat.from_images(image_path, resolution = 2)\n",
    "# Check that the polydat object has been created correctly.\n",
    "print(f'The first polydat object contains {len(polydat.images)} image(s).')\n",
    "\n",
    "# Example of creating an instance of the Polydat class from a list of image file paths.\n",
    "image_paths = ['example_images/exampleCL0.png', 'example_images/exampleCL1.png']\n",
    "polydat = PsPolypy.Polymer.Polydat.from_images(image_paths, resolution = 2)\n",
    "# Check that the polydat object has been created correctly.\n",
    "print(f'The second polydat object contains {len(polydat.images)} image(s).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty instance of the Polydat class and add an image to it.\n",
    "polydat = PsPolypy.Polymer.Polydat()\n",
    "polydat.add_image('example_images/exampleCL0.png', resolution = 2)\n",
    "polydat.add_image('example_images/exampleCL1.png', resolution = 2)\n",
    "# Check that the polydat object has been created correctly.\n",
    "print(f'The polydat object contains {len(polydat.images)} image(s).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Polydat class from a list of numpy arrays.\n",
    "filepath = 'example_images/exampleCL0.png'\n",
    "image = io.imread(filepath)\n",
    "image_data = img_as_float(rgba2rgb(image))\n",
    "polydat = PsPolypy.Polymer.Polydat([image_data], resolution = 2)\n",
    "# Check that the polydat object has been created correctly.\n",
    "print(f'The polydat object contains {len(polydat.images)} image(s).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upscaling images with Polydat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method for processing the image(s) is to (optionally) upscale them. The ```upscale``` method takes two inputs, ```magnification``` (int/float) and ```order``` (int). Each image has its shape multiplied by the ```magnification``` factor and interpolated by a 2D spline of ```order``` power. The upscaled image is set set back in the ```images``` attribute, i.e. this operation occurs in place. Furthermore, the ```resolution``` attribute is dynamically updated to reflect the new nm/px value. \n",
    "\n",
    "Note:\n",
    "\n",
    "```upscale``` method raises an exception if the ```upscaled``` metadata is ```True```. This prevents upscaled images from being upscaled multiple times. After ```upscale``` is executing the ```metadata``` will be automatically updated with ```upscaled: True```. \n",
    "\n",
    "Though ```magnification```may be a floating point number, this may lead to warped images and unexpected behavior; it is preferrable to keep the magnification factor an integer. The interpolation ```order``` must be an integer from 0 to 5, indicating nearest-neighbor to bi-quintic interpolation. The default interpolation order is 3: bi-cubic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Polydat class from the test image.\n",
    "image_path = ['example_images/exampleCL0.png']\n",
    "polydat = PsPolypy.Polymer.Polydat.from_images(image_path, resolution = 2)\n",
    "# Extract the image from the polydat object.\n",
    "base_image = polydat.images[0]\n",
    "\n",
    "# Upscale the image by a factor of 2 using bi-cubic interpolation.\n",
    "polydat.upscale(magnification = 2, order = 3)\n",
    "# Extract the upscaled image from the polydat object.\n",
    "upscaled_image = polydat.images[0]\n",
    "\n",
    "# Plot the original and upscaled images for comparison.\n",
    "fig, ax = plt.subplots(1,2, figsize = (7,6))\n",
    "for axis in ax:\n",
    "    axis.axis('off')\n",
    "\n",
    "# Display the images.\n",
    "ax[0].imshow(base_image, cmap = 'gray')\n",
    "ax[0].set_title(f'Original {base_image.shape}')\n",
    "ax[1].imshow(upscaled_image, cmap = 'gray')\n",
    "ax[1].set_title(f'Upscaled {upscaled_image.shape}')\n",
    "\n",
    "# Display the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```segment_particles``` method segments the images in the ```polydat``` instance. This method takes two optional arguments: ```minimum_area``` (int) and ```padding``` (int). During segmentation, any particle whose area is less than ```minimum_area``` is discarded. ```padding``` indicates how many padding pixels to include around the cropped region surrounding each particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment the particles in the image.\n",
    "polydat.segment_particles()\n",
    "\n",
    "# Check to see how many particles were detected.\n",
    "print(f'The polydat object detected and segmented {polydat.num_particles} particles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segmented particles can be visualized with ```plot_particle```. It takes the argument ```index``` for which particle to plot.\n",
    "\n",
    "**Note:** All plotting functions take the optional argument ```ax``` which chooses the ```matplotlib.pyplot.axis``` to draw to. If not explicitly specified, the current axis is selected with ```matplotlib.pyplot.gca```. For more information, see the Matplotlib documentation [here](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.gca.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which particle to view the image of.\n",
    "particle_index = 10\n",
    "\n",
    "# Create the figure and the axis for the particle image.\n",
    "particle_fig, ax = plt.subplots(figsize = (6,6))\n",
    "# Plot the image to the axis.\n",
    "polydat.plot_particle(particle_index, ax, cmap = 'gray')\n",
    "# Set the title of the plot.\n",
    "ax.set_title(f'Particle {particle_index} Image')\n",
    "# Turn the axis off\n",
    "ax.axis('off')\n",
    "# Display the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```skeletoinize_particles``` method creates skan skeleton representations of each ```particle```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeletonize the particles with the default parameters.\n",
    "polydat.skeletonize_particles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skeletons can be visualized with ```plot_skeleton```. It takes the argument ```index``` for which skeleton to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which particle to view the skeleton of.\n",
    "particle_index = 10\n",
    "\n",
    "# Create the figure and the axis for the skeleton image.\n",
    "skeleton_fig1, ax = plt.subplots(figsize = (6,6))\n",
    "# Plot the skeleton image to the axis.\n",
    "ax = polydat.plot_skeleton(particle_index, cmap = 'gray')\n",
    "# Set the title of the plot.\n",
    "ax.set_title(f'Particle {particle_index} Skeleton')\n",
    "# Turn the axis off\n",
    "ax.axis('off')\n",
    "# Display the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```interpolate_skeletons``` interpolates each particle skeleton along the contur in a user defined ```step_size``` (float). ```step_size``` can be set below 1 to sample the contour above the current pixel resolution. The optional arguments for the interpolation order ```k``` (int) and smoothing ```s``` (float) are passed to ```scipy.interpolate.splprep```. For more information see the scipy documentation [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.splprep.html#scipy.interpolate.splprep).\n",
    "\n",
    "```plot_interpolated_skeleton``` will now display the interpolated skeleton contour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate the skeletons of the particles.\n",
    "polydat.interpolate_skeletons(step_size = 0.5, k = 3, s = 0.5)\n",
    "\n",
    "# Select which particle to view the interpolated skeleton of.\n",
    "particle_index = 10\n",
    "\n",
    "# Create the figure and the axis for the skeleton image.\n",
    "skeleton_fig2, ax = plt.subplots(figsize = (6,6))\n",
    "# Plot the skeleton image to the axis.\n",
    "polydat.plot_skeleton(particle_index, ax, cmap = 'gray',alpha=1)\n",
    "# Plot the interpolated skeleton to the axis.\n",
    "polydat.plot_interpolated_skeleton(particle_index, ax, lw = 6, color = 'red')\n",
    "# Set the title of the plot.\n",
    "ax.set_title(f'Particle {particle_index} Interpolated Skeleton')\n",
    "# Turn the axis off\n",
    "ax.axis('off')\n",
    "# Display the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```plot_contour_distribution``` plots a distribution of all particle contour lengths. This plotting takes in ```Dict```s of keyword arguments passed to matplotlib. The kwargs for controling the plot are ```inc_dist_kwargs``` and ```inc_fill_kwargs``` for the distribtuion and fill respectively. See the docstring for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure and the axis for the contour distribution.\n",
    "contour_distribution_fig1, ax = plt.subplots(figsize = (6,6))\n",
    "\n",
    "# Plot the contour length distribution.\n",
    "ax = polydat.plot_contour_distribution(n_points = 1000,\n",
    "                                       inc_dist_kwargs = {'color': 'Blue', 'lw': 2,},\n",
    "                                       inc_fill_kwargs = {'color': 'LightBlue', 'alpha': 0.5})\n",
    "# Set the title of the plot.\n",
    "ax.set_title('Contour Length Distribution')\n",
    "# Set the axis labels.\n",
    "ax.set_xlabel(fr'Contour Length [$\\times${polydat.resolution:.0f} nm]')\n",
    "ax.set_ylabel('Probability Density')\n",
    "# Set the axis limits.\n",
    "ax.set_ylim([0,0.085])\n",
    "ax.set_xlim([0,75])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying The Particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```classify_particles``` sets the ```classification``` attribute of each particle. Each particle is classified by the following criteria:\n",
    "- If the particle contains a single branch path with two different endpoints, the particle is classified as ```Linear```.\n",
    "- If the particle contains a single branch path with the same end points, the particle is classified as ```Looped```.\n",
    "- If the particle contains multiple branch paths that intersect at a branch point but no cycles (A point can be revisited when traveling along a set of branch paths), the particle is classified as ```Branched```.\n",
    "- If the particle contains multiple branch paths that intersect at a branch point and includes cycles, the particle is classified as ```Brached-Looped```.\n",
    "- If none of the above criteria are met, the particle is classified as ```Unknown```. This should not occur, but is included for safety.\n",
    "\n",
    "```filter_particles``` takes the argument ```classifications``` (list[str]). This removes all particles from the ```polydat``` instance whose classification is not in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the Particles\n",
    "polydat.classify_particles()\n",
    "\n",
    "# Get the number of particles in the Linear classification.\n",
    "linear_particles = [particle for particle in polydat.particles if particle.classification == 'Linear']\n",
    "print(f'The number of linear particles is {len(linear_particles)}.')\n",
    "\n",
    "# Select the first particle in the Linear classification.\n",
    "particle = linear_particles[0]\n",
    "\n",
    "# Create the figure and the axis for the particle image.\n",
    "linear_skeleton_fig, ax = plt.subplots(figsize = (6,6))\n",
    "# Plot the first particle's skeleton.\n",
    "ax = particle.plot_skeleton(cmap = 'gray')\n",
    "# Plot the first particle's interpolated skeleton.\n",
    "ax = particle.plot_interpolated_skeleton(lw = 6, color = 'red')\n",
    "# Set the title of the plot.\n",
    "ax.set_title(f'Particle {particle.id} Interpolated Skeleton - {particle.classification}')\n",
    "# Turn the axis off\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Persistence Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```calc_tantan_correlations``` calculates the Tangent-Tangent correlation for all pairs of points in all paths. The correlations are averaged for each contour length and ```mean_tantan_correlation``` is set.. \n",
    "\n",
    "```plot_mean_tantan_correlation``` creates a plot of the mean Tangent-Tangent correlation. It takes a single optional argument ```error_bars``` (bool) to turn error bars on and off. Furthermore, the plotting function includes keyword argument dictionaries ```inc_kwargs``` and ```exc_kwargs``` for altering the color of the correlation, its errorbars, etc. See the source code for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the tangent-tangent correlation functions for all the particles.\n",
    "polydat.calc_tantan_correlations()\n",
    "\n",
    "tantan_correlation_fig1, ax = plt.subplots(figsize = (6,6))\n",
    "# Plot the mean tangent-tangent correlation function.\n",
    "polydat.plot_mean_tantan_correlations(error_bars = True,\n",
    "                                          inc_kwargs = {'color': 'Blue', 'fmt': '.', 'ecolor': 'LightBlue'},\n",
    "                                          exc_kwargs = {'color': 'Gray', 'fmt': '.', 'ecolor': 'LightGray'})\n",
    "# Set the title of the plot.\n",
    "ax.set_title('Mean Tangent-Tangent Correlation')\n",
    "# Set the axis labels.\n",
    "ax.set_xlabel(fr'$s\\; [\\times{polydat.resolution:.0f} nm]$',fontsize=14)\n",
    "ax.set_ylabel(r'$|\\langle\\cos\\theta\\rangle|=|\\langle\\vec{t}(s)\\cdot\\vec{t}(0)\\rangle|$',fontsize=14)\n",
    "# Set the axis limits.\n",
    "ax.set_xlim(0,40)\n",
    "ax.set_ylim(0,1.05)\n",
    "# Turn the grid on.\n",
    "ax.grid()\n",
    "# Display the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```plot_subpath_contour_distribution``` plots a distribution of all particle subpath contour lengths. This plotting takes in ```Dict```s of keyword arguments passed to matplotlib. The kwargs for controling the plot are ```inc_dist_kwargs``` and ```inc_fill_kwargs``` for the distribtuion and fill respectively. See the docstring for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure and the axis for the contour distribution.\n",
    "subpath_contour_distribution_fig, ax = plt.subplots(figsize = (6,6))\n",
    "\n",
    "# Plot the contour length distribution.\n",
    "ax = polydat.plot_subpath_contour_distribution(n_points = 1000,\n",
    "                                               inc_dist_kwargs = {'color': 'Blue', 'lw': 2,},\n",
    "                                               inc_fill_kwargs = {'color': 'LightBlue', 'alpha': 0.5})\n",
    "# Set the title of the plot.\n",
    "ax.set_title('Subpath Contour Length Distribution')\n",
    "# Set the axis labels.\n",
    "ax.set_xlabel(fr'Contour Length [$\\times${polydat.resolution:.0f} nm]')\n",
    "ax.set_ylabel('Probability Density')\n",
    "# Set the axis limits.\n",
    "ax.set_ylim([0,0.085])\n",
    "ax.set_xlim([0,75])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```calc_tantan_lp``` fits the mean Tangent-Tangent correlation to calculate the persistence length. It accepts two optional arguments, ```min_fitting_length``` (float) and ```max_fitting_length``` (float). Because the correlation contribution from low contour lengths can bias the fit, and the sampling of very long contour lengths is very low, the decaying exponential is only fit between these two values. Executing ```calc_tantan_lp``` sets the ```tantan_fit_result``` parmeter with a ```lmfit.model.ModelResult```. For more information about the result see the documentation [here](https://lmfit.github.io/lmfit-py/model.html#lmfit.model.ModelResult).\n",
    "\n",
    "```print_summary``` prints a summary of the analysis performed. ```plot_mean_tantan_correlation_fit``` plots the fitted exponential. If the ```show_init``` argument is ```True```, the initial guess at the fit is shown along side the best fit. The ```init_kwargs``` can be set to adjust the parameters of the initial guess plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the persistence length of the particles.\n",
    "L_min = 10\n",
    "L_max = 30\n",
    "polydat.calc_tantan_lp(min_fitting_length = L_min, max_fitting_length = L_max)\n",
    "\n",
    "# Print a summary of the polydat object.\n",
    "polydat.print_summary()\n",
    "\n",
    "tantan_correlation_fig2, ax = plt.subplots(figsize = (8,7))\n",
    "# Plot the mean tangent-tangent correlation function.\n",
    "polydat.plot_mean_tantan_correlations(ax = ax, error_bars = True,\n",
    "                                          inc_kwargs = {'color': 'Blue', 'fmt': '.', 'ecolor': 'Blue', 'lw': 0.7, 'label': 'Fitted Data'},\n",
    "                                          exc_kwargs = {'color': 'Gray', 'fmt': '.', 'ecolor': 'Gray', 'lw': 0.7, 'label': 'Excluded Data'},\n",
    "                                          vline_kwargs = {'color': 'Blue', 'lw': 0.75, 'dashes': [8,3]})\n",
    "# Plot the fitted decaying exponential.\n",
    "polydat.plot_mean_tantan_correlations_fit(ax = ax,show_init = False,\n",
    "                                              fit_kwargs = {'color': 'Red', 'lw': 1.5, 'label': 'Best Fit'},\n",
    "                                              init_kwargs = {'color': 'Red', 'lw': 0.75, 'linestyle': '--', 'label': 'Initial Guess'})\n",
    "\n",
    "# Set the title of the plot.\n",
    "# ax.set_title('Mean Tangent-Tangent Correlation')\n",
    "\n",
    "# Set the axis labels.\n",
    "ax.set_xlabel(rf'$s\\;[\\times{polydat.resolution:.0f} nm]$',fontsize=14)\n",
    "ax.set_ylabel(r'$|\\langle\\cos\\theta(s)\\rangle|$',fontsize=14)\n",
    "\n",
    "# Set legend\n",
    "ax.legend(markerscale=0.8,fontsize=12)\n",
    "\n",
    "# Set the axis limits.\n",
    "ax.set_xlim(0,40)\n",
    "ax.set_ylim(0,1.1)\n",
    "# Turn the grid on.\n",
    "ax.grid()\n",
    "# Display the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tangent-Tangent Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of the end-to-end workflow for processing a set of polymer images for all classification types using the Tangent-Tangent Correlation Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the files to be analyzed.\n",
    "filepaths = ['example_images/exampleCL0.png', 'example_images/exampleCL1.png']\n",
    "\n",
    "# Create an instance of the Polydat class from the list of file paths.\n",
    "polydat = PsPolypy.Polymer.Polydat.from_images(filepaths = filepaths, resolution = 2)\n",
    "\n",
    "# Upscale the image by a factor of 2 using bi-cubic interpolation.\n",
    "polydat.upscale(magnification = 2, order = 3)\n",
    "\n",
    "# Segment the particles in the image.\n",
    "polydat.segment_particles()\n",
    "\n",
    "# Skeletonize the particles.\n",
    "polydat.skeletonize_particles()\n",
    "\n",
    "# Classify the particles.\n",
    "polydat.classify_particles()\n",
    "\n",
    "# Interpolate the skeletons of the particles.\n",
    "polydat.interpolate_skeletons(step_size = 0.5, k = 3, s = .5)\n",
    "\n",
    "# Calculate the tangent-tangent correlation for the particles.\n",
    "polydat.calc_tantan_correlations()\n",
    "\n",
    "# Calculate the persistence length.\n",
    "L_min = 10\n",
    "L_max = 30\n",
    "polydat.calc_tantan_lp(lp_init = 11, min_fitting_length = L_min, max_fitting_length = L_max, scale_covar = True)\n",
    "\n",
    "# Print a summary of the polydat object.\n",
    "polydat.print_summary()\n",
    "\n",
    "# Create a figure containing the complete workflow plots.\n",
    "fig_ttc, ax = plt.subplots(2,1, figsize = (8,8),sharex=True)\n",
    "    \n",
    "# Plot contour length distribution\n",
    "polydat.plot_subpath_contour_distribution(ax = ax[0], n_points = 1000,\n",
    "                                  inc_dist_kwargs = {'color': 'Blue', 'lw': 2, 'label': 'Included data'},\n",
    "                                  inc_fill_kwargs = {'color': 'LightBlue', 'alpha': 0.5},\n",
    "                                  exc_dist_kwargs = {'color': 'Gray', 'lw': 2, 'alpha': 0.5, 'label': 'Excluded data'},\n",
    "                                  exc_fill_kwargs = {'color': 'LightGray', 'alpha': 0.5},\n",
    "                                  vline_kwargs = {'color': 'Blue', 'lw': 0.7, 'dashes': [8,3]})\n",
    "\n",
    "ax[0].set_ylabel(r'$P(s)$',fontsize=18)\n",
    "ax[0].set_xlim([0,60])\n",
    "ax[0].set_ylim([0,0.085])\n",
    "ax[0].grid(lw=0.3)\n",
    "ax[0].legend(fontsize = 14)\n",
    "\n",
    "# Plot tangent-tangent correlation with the fit.\n",
    "polydat.plot_mean_tantan_correlations(error_bars = True, ax = ax[1],\n",
    "                                     inc_kwargs = {'color': 'Blue', 'fmt': '.', 'ecolor': 'LightBlue', 'label': 'Fitted Data'},\n",
    "                                     exc_kwargs = {'color': 'Gray', 'fmt': '.', 'ecolor': 'LightGray', 'label': 'Excluded Data'})\n",
    "polydat.plot_mean_tantan_correlations_fit(ax = ax[1],\n",
    "                                         fit_kwargs = {'color': 'Red', 'lw': 1.5, 'label': 'Best Fit'},\n",
    "                                         init_kwargs = {'color': 'Red', 'lw': 0.75, 'linestyle': '--', 'label': 'Initial Guess'})\n",
    "\n",
    "ax[1].set_xlabel(rf'$s\\;[\\times{polydat.resolution:.0f} nm]$',fontsize=18)\n",
    "ax[1].set_ylabel(r'$|\\langle\\cos\\theta(s)\\rangle|$',fontsize=18)\n",
    "ax[1].set_xlim([0,50])\n",
    "ax[1].set_ylim(0,1.05)\n",
    "ax[1].grid(lw=0.3)\n",
    "ax[1].legend(fontsize=14)\n",
    "\n",
    "# Display the plot.\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Show the fitting results.\n",
    "# polydat.tantan_fit_result\n",
    "\n",
    "\n",
    "# Extract best-fit parameter and its standard deviation\n",
    "result = polydat.tantan_fit_result\n",
    "lp_0 = result.params['lp'].value\n",
    "std_0 = result.params['lp'].stderr\n",
    "\n",
    "# Goodness-of-fit measure (reduced chi-squared)\n",
    "rchi2_0 = result.redchi  # Reduced chi-squared\n",
    "\n",
    "# Scale the standard deviation only if reduced chi-squared is greater than 1\n",
    "if rchi2_0 > 1.0:\n",
    "    scaled_std_r2 = np.sqrt(rchi2_0) * std_0\n",
    "else:\n",
    "    scaled_std_r2 = std_0\n",
    "\n",
    "# Display results\n",
    "display(Math(rf\"$\\chi_{{red}}^2 = {rchi2_0:.2f}$\"))\n",
    "print('-' * 32)\n",
    "display(Math(rf\"$l_p = {lp_0:.1f} \\pm {scaled_std_r2:.1f}$\"))\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\langle R^2\\rangle$ - Mean Square End-To-End Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above workflow can be performed to calculate the Mean Squared End-To-End distance and fit. Simply replace ```calc_tantan_correlations``` and ```calc_tantan_lp``` with ```calc_displacements``` and ```calc_R2_lp``` respectively. Below is an example of the end-to-end workflow for processing a set of polymer images for only linear particles using the Mean Squared End to End Distance Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the files to be analyzed.\n",
    "filepaths = ['example_images/exampleCL0.png', 'example_images/exampleCL1.png']\n",
    "\n",
    "# Create an instance of the Polydat class from the list of file paths.\n",
    "polydat = PsPolypy.Polymer.Polydat.from_images(filepaths = filepaths, resolution = 2)\n",
    "\n",
    "# Upscale the image by a factor of 2 using bi-cubic interpolation.\n",
    "polydat.upscale(magnification = 2, order = 3)\n",
    "\n",
    "# Segment the particles in the image.\n",
    "polydat.segment_particles()\n",
    "\n",
    "# Skeletonize the particles.\n",
    "polydat.skeletonize_particles()\n",
    "\n",
    "# Classify the particles.\n",
    "polydat.classify_particles()\n",
    "\n",
    "# Filter the particles to only include the Linear classification.\n",
    "polydat.filter_particles(classifications = ['Linear'])\n",
    "\n",
    "# Interpolate the skeletons of the particles.\n",
    "polydat.interpolate_skeletons(step_size = 0.5, k = 3, s = .5)\n",
    "\n",
    "# Calculate <R^2>\n",
    "polydat.calc_displacements()\n",
    "\n",
    "L_min = 10\n",
    "L_max = 40\n",
    "# Calculate the persistence length.\n",
    "polydat.calc_R2_lp(lp_init = 11, min_fitting_length = L_min, max_fitting_length = L_max, scale_covar = True)\n",
    "\n",
    "# Print a summary of the polydat object.\n",
    "polydat.print_summary()\n",
    "\n",
    "# Create a figure containing the complete workflow plots.\n",
    "fig_R2, ax = plt.subplots(2,1, figsize = (8,8),sharex=True)\n",
    "\n",
    "# Plot contour length distribution\n",
    "ax[0] = polydat.plot_subpath_contour_distribution(ax = ax[0], n_points = 1000,\n",
    "                                          inc_dist_kwargs = {'color': 'Blue', 'lw': 2, 'label': 'Included Distribution'},\n",
    "                                          inc_fill_kwargs = {'color': 'LightBlue', 'alpha': 0.5},\n",
    "                                          exc_dist_kwargs = {'color': 'Gray', 'lw': 2, 'alpha': 0.5, 'label': 'Excluded Distribution'},\n",
    "                                          exc_fill_kwargs = {'color': 'LightGray', 'alpha': 0.5},\n",
    "                                          vline_kwargs = {'color': 'Blue', 'lw': 0.75, 'dashes': [8,3]})\n",
    "\n",
    "ax[0].set_ylim(0,0.06)\n",
    "ax[0].set_ylabel(r'$P(L)$',fontsize=18)\n",
    "ax[0].grid(lw=0.3)\n",
    "ax[0].legend(fontsize = 14)\n",
    "\n",
    "# Plot tangent-tangent correlation with the fit.\n",
    "polydat.plot_mean_squared_displacements(ax = ax[1], error_bars = True,\n",
    "                                        inc_kwargs = {'color': 'Blue', 'fmt': '.', 'ecolor': 'LightBlue', 'label': 'Fitted Data'},\n",
    "                                        exc_kwargs = {'color': 'Gray', 'fmt': '.', 'ecolor': 'LightGray', 'label': 'Excluded Data'})\n",
    "polydat.plot_mean_squared_displacements_fit(ax = ax[1], show_init = False,\n",
    "                                            fit_kwargs = {'color': 'Red', 'lw': 1.5, 'label': 'Best Fit'},\n",
    "                                            init_kwargs = {'color': 'Red', 'lw': 0.75, 'linestyle': '--', 'label': 'Initial Guess'})\n",
    "ax[1].set_xlim([0,50])\n",
    "ax[1].set_ylim(0,2200)\n",
    "ax[1].set_xlabel(rf'$L\\;[\\times{polydat.resolution:.0f} nm]$',fontsize=18)\n",
    "ax[1].set_ylabel(r'$\\langle R^2(L)\\rangle$',fontsize=18)\n",
    "\n",
    "\n",
    "ax[1].grid(lw=0.3)\n",
    "ax[1].legend(fontsize = 14)\n",
    "\n",
    "# Display the plot.\n",
    "plt.show()\n",
    "# Show the fitting results.\n",
    "# polydat.wlc_fit_result\n",
    "\n",
    "\n",
    "# Extract best-fit parameter and its standard deviation\n",
    "result = polydat.R2_fit_result\n",
    "lp_0 = result.params['lp'].value\n",
    "std_0 = result.params['lp'].stderr\n",
    "\n",
    "# Goodness-of-fit measure (reduced chi-squared)\n",
    "rchi2_0 = result.redchi  # Reduced chi-squared\n",
    "\n",
    "# Scale the standard deviation only if reduced chi-squared is greater than 1\n",
    "if rchi2_0 > 1.0:\n",
    "    scaled_std_r2 = np.sqrt(rchi2_0) * std_0\n",
    "else:\n",
    "    scaled_std_r2 = std_0\n",
    "\n",
    "# Display results\n",
    "display(Math(rf\"$\\chi_{{red}}^2 = {rchi2_0:.2f}$\"))\n",
    "print('-' * 32)\n",
    "display(Math(rf\"$l_p = {lp_0:.1f} \\pm {scaled_std_r2:.1f}$\"))\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versions of used packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark --python --packages numpy,scipy,networkx,skan,matplotlib,PIL,skimage,lmfit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PsPolypy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
